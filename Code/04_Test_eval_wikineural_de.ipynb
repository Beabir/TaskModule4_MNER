{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a6d5f55-0767-4e2d-b6e5-60c42d024dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import classification_report\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c8b94de-78c6-4153-9803-4743ba2f5cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model and tokenizer\n",
    "model_name = \"./finetuned_wikineural\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3fa4850-0346-4410-a716-82132d621367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed French dataset\n",
    "from datasets import load_from_disk\n",
    "german_dataset = load_from_disk(\"Data_de/test_de\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "407e7fb5-60ac-464a-b2fb-ce92a26d8c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['WEITERLEITUNG', 'Hu', 'Xian'], 'ner_tags': [0, 5, 6, 6, 6], 'langs': ['de', 'de', 'de', 'de', 'de'], 'spans': ['LOC: Hu ( Xi’an )'], 'input_ids': [101, 160, 11259, 37611, 24093, 51036, 37611, 69849, 11447, 43707, 59876, 10206, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, 0, -100, -100, -100, -100, -100, -100, -100, 5, 6, -100, -100]}\n",
      "{'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'ner_tags': Sequence(feature=ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC'], id=None), length=-1, id=None), 'langs': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'spans': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'token_type_ids': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None), 'labels': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None)}\n"
     ]
    }
   ],
   "source": [
    "print(german_dataset[0])  # Print the first example\n",
    "print(german_dataset.features)  # Print the dataset features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26a720ed-f3f4-4e62-83bc-4a53d63d2367",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(examples):\n",
    "    # The tokenization step is not needed as input_ids are already present\n",
    "    return {\n",
    "        'input_ids': examples['input_ids'],\n",
    "        'attention_mask': examples['attention_mask'],\n",
    "        'token_type_ids': examples['token_type_ids'],\n",
    "        'labels': examples['labels']\n",
    "    }\n",
    "\n",
    "tokenized_german = german_dataset.map(prepare_dataset, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc16a5a5-323c-425e-bc13-f75f88a9e82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data loader:\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'input_ids': torch.tensor([item['input_ids'] for item in batch]),\n",
    "        'attention_mask': torch.tensor([item['attention_mask'] for item in batch]),\n",
    "        'token_type_ids': torch.tensor([item['token_type_ids'] for item in batch]),\n",
    "        'labels': torch.tensor([item['labels'] for item in batch])\n",
    "    }\n",
    "\n",
    "data_loader = DataLoader(tokenized_german, batch_size=16, shuffle=False, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d5cc0a8-17e7-44f3-b4d2-792928694379",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = [torch.tensor(item['input_ids']) for item in batch]\n",
    "    attention_mask = [torch.tensor(item['attention_mask']) for item in batch]\n",
    "    token_type_ids = [torch.tensor(item['token_type_ids']) for item in batch]\n",
    "    labels = [torch.tensor(item['labels']) for item in batch]\n",
    "    \n",
    "    # Pad sequences to the maximum length in the batch\n",
    "    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=0)\n",
    "    attention_mask = pad_sequence(attention_mask, batch_first=True, padding_value=0)\n",
    "    token_type_ids = pad_sequence(token_type_ids, batch_first=True, padding_value=0)\n",
    "    labels = pad_sequence(labels, batch_first=True, padding_value=-100)  # Use -100 for padding in labels\n",
    "    \n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'token_type_ids': token_type_ids,\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "data_loader = DataLoader(tokenized_german, batch_size=16, shuffle=False, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7594bca1-7a7e-406c-af12-13f3e8bcff00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 625/625 [01:34<00:00,  6.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.25      0.30      0.27      4189\n",
      "         ORG       0.22      0.21      0.22      4884\n",
      "         PER       0.21      0.19      0.19      5839\n",
      "\n",
      "   micro avg       0.22      0.23      0.22     14912\n",
      "   macro avg       0.22      0.23      0.23     14912\n",
      "weighted avg       0.22      0.23      0.22     14912\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from seqeval.metrics import classification_report\n",
    "import torch\n",
    "\n",
    "model.eval()\n",
    "all_predictions = []\n",
    "all_true_labels = []\n",
    "\n",
    "label_map = {0: 'O', 1: 'B-PER', 2: 'I-PER', 3: 'B-ORG', 4: 'I-ORG', 5: 'B-LOC', 6: 'I-LOC'}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "        # Überprüfe, ob alle Keys vorhanden sind\n",
    "        assert 'input_ids' in batch and 'attention_mask' in batch and 'labels' in batch, \\\n",
    "            \"Batch fehlt erforderliche Schlüssel!\"\n",
    "\n",
    "        # Inputs und Labels in das richtige Device laden\n",
    "        inputs = {k: v.to(model.device) for k, v in batch.items() if k != 'labels'}\n",
    "        labels = batch['labels'].to(model.device)\n",
    "        \n",
    "        # Vorhersagen generieren\n",
    "        outputs = model(**inputs)\n",
    "        predictions = torch.argmax(outputs.logits, dim=2)\n",
    "        \n",
    "        # Labels und Vorhersagen verarbeiten\n",
    "        for pred, label, mask in zip(predictions, labels, batch['attention_mask']):\n",
    "            pred = pred.cpu().numpy()\n",
    "            label = label.cpu().numpy()\n",
    "            mask = mask.cpu().numpy()\n",
    "\n",
    "            # Filtern mit der attention_mask\n",
    "            true_label = [label_map.get(l, 'O') for l, m in zip(label, mask) if m != 0]\n",
    "            pred_label = [label_map.get(p, 'O') for p, m in zip(pred, mask) if m != 0]\n",
    "            \n",
    "            # Listenlänge angleichen (falls nötig)\n",
    "            if len(true_label) != len(pred_label):\n",
    "                min_len = min(len(true_label), len(pred_label))\n",
    "                true_label = true_label[:min_len]\n",
    "                pred_label = pred_label[:min_len]\n",
    "            \n",
    "            all_predictions.append(pred_label)\n",
    "            all_true_labels.append(true_label)\n",
    "\n",
    "# Klassifikationsbericht ausgeben\n",
    "print(classification_report(all_true_labels, all_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3e0e8a4e-320d-4f64-8de9-6ba6af5fd724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: Star, Label: B-ORG\n",
      "Token: ##bu, Label: I-ORG\n",
      "Token: ##cks, Label: I-ORG\n",
      "Token: Bern, Label: B-LOC\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Modell und Tokenizer laden (ersetze den Modellpfad mit deinem Fine-Tuned Modell)\n",
    "model_name = \"./finetuned_wikineural\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "\n",
    "# Zu analysierender Beispielsatz\n",
    "sentence = \"Julia arbeitet bei Starbucks in Bern.\"\n",
    "\n",
    "# Tokenisierung\n",
    "inputs = tokenizer(\n",
    "    sentence,\n",
    "    return_tensors=\"pt\",  # Gibt PyTorch-Tensoren zurück\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    is_split_into_words=False  # Falls Satz schon tokenisiert, dann True\n",
    ")\n",
    "\n",
    "# Inference (Modellvorhersage)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Logits in Vorhersagen umwandeln\n",
    "logits = outputs.logits\n",
    "predictions = torch.argmax(logits, dim=2).squeeze().tolist()\n",
    "\n",
    "# Token-IDs in Tokens umwandeln\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"].squeeze().tolist())\n",
    "\n",
    "# Mapping der Label-IDs auf ihre Bedeutungen (ersetze durch dein Label-Mapping)\n",
    "label_map = {0: 'O', 1: 'B-PER', 2: 'I-PER', 3: 'B-ORG', 4: 'I-ORG', 5: 'B-LOC', 6: 'I-LOC'}\n",
    "\n",
    "# Tokens den Labels zuordnen\n",
    "results = []\n",
    "for token, prediction in zip(tokens, predictions):\n",
    "    label = label_map[prediction]\n",
    "    results.append((token, label))\n",
    "\n",
    "# Ergebnisse filtern (nur relevante Tokens anzeigen)\n",
    "filtered_results = [(token, label) for token, label in results if label != 'O' and token not in tokenizer.all_special_tokens]\n",
    "\n",
    "# Ergebnisse anzeigen\n",
    "for token, label in filtered_results:\n",
    "    print(f\"Token: {token}, Label: {label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5b2f8516-b3a1-439b-9457-079901bbc1d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Julia', 'arbeitet', 'bei', 'Star', '##bu', '##cks', 'in', 'Bern', '.']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.tokenize(sentence))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1b5c3183-a4ba-4802-bd14-5ceeebfa73ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: [CLS], Label: O\n",
      "Token: Julia, Label: O\n",
      "Token: arbeitet, Label: O\n",
      "Token: bei, Label: O\n",
      "Token: Star, Label: B-ORG\n",
      "Token: ##bu, Label: I-ORG\n",
      "Token: ##cks, Label: I-ORG\n",
      "Token: in, Label: O\n",
      "Token: Bern, Label: B-LOC\n",
      "Token: ., Label: O\n",
      "Token: [SEP], Label: O\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "outputs = model(**inputs)\n",
    "logits = outputs.logits\n",
    "predictions = torch.argmax(logits, dim=2)\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"].squeeze().tolist())\n",
    "predicted_labels = [label_map[pred.item()] for pred in predictions.squeeze()]\n",
    "\n",
    "# Mapping der Tokens zu Labels\n",
    "for token, label in zip(tokens, predicted_labels):\n",
    "    print(f\"Token: {token}, Label: {label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4ecf1f-c9b4-4099-9db9-c8489565c982",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
